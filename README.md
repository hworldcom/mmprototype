# Avellaneda–Stoikov Market Making Research Project

## Objective

The goal of this project is to **research, test, and validate market‑making models**, with a primary focus on the **Avellaneda–Stoikov framework**, before any production deployment.

This repository is intentionally structured as a **research pipeline**, not a ready‑made trading bot.

---

## Why This Project Exists

Market making models depend critically on:
- execution uncertainty
- inventory risk
- order arrival dynamics
- market microstructure

Without realistic data and fill assumptions, theoretical results are misleading.
This project exists to close that gap.

---

## What Is Required to Test Market‑Making Models

To correctly test an Avellaneda–Stoikov‑style strategy we need:

- L2 order book states (bids & asks)
- Executed trade flow
- Exchange timestamps
- Robust fill models
- Deterministic backtesting

---

## Repository Skeleton

```
.
├── README.md
├── requirements.txt
├── Dockerfile / Makefile
├── data/                  # Recorder output (per symbol/day). Shared with backtests.
├── mm/
│   ├── market_data/
│   │   ├── README.md
│   │   ├── recorder.py
│   │   ├── sync_engine.py
│   │   ├── local_orderbook.py
│   │   ├── buffered_writer.py
│   │   ├── ws_stream.py
│   │   └── snapshot.py
│   ├── calibration/
│   │   ├── runner_calibration.py
│   │   ├── exposure.py
│   │   ├── poisson_fit.py
│   │   └── quotes/
│   └── backtest/
│       ├── README.md
│       ├── replay.py
│       ├── paper_exchange.py
│       ├── io.py
│       ├── fills/
│       └── quotes/
├── tests/                 # Recorder and sync-engine unit tests
└── logs/, restore/, etc.  # Environment-specific artifacts
```

`mm/market_data` is the producer of historical datasets; `mm/backtest` consumes them for deterministic replay. Both subtrees have their own README files referenced below.

---

## Current State (Implemented)

### Repository Structure

| Path | Purpose | Key dependency |
|------|---------|----------------|
| `mm/market_data/` | Real-time recorder that captures Binance Spot depth/trade streams, reconstructs the local order book, and writes CSV / NDJSON artifacts plus an events ledger. | Consumed by `mm/backtest`, documented in `mm/market_data/README.md`. |
| `mm/calibration/` | Parameter calibration utilities that *produce* inputs for later backtests (e.g. Poisson A,k). | Consumes recorder outputs and emits JSON/CSV artifacts under `out/calibration/`. |
| `mm/backtest/` | Offline replay, paper exchange, and fill-model experiments that operate on recorded data. | Requires recorder outputs (same folder layout) at runtime; documented in `mm/backtest/README.md`. |

These subtrees now ship with their own READMEs for day-to-day usage, whereas this root README stays focused on the overarching goals and roadmap.

---

## Run Logs (Backtest + Calibration)

Batch jobs (backtests and calibration runs) write **run-scoped logs** to make debugging deterministic.

Default layout:

```
out/logs/
  backtest/<SYMBOL>/<YYYYMMDD>/<RUN_ID>/run.log
  calibration/<METHOD>/<SYMBOL>/<YYYYMMDD>/<RUN_ID>/run.log
```

Controls:
- `LOG_LEVEL` (default: `INFO`; set `DEBUG` when investigating)
- `LOG_ROOT` (default: `out/logs`)
- `RUN_ID` (default: autogenerated UTC timestamp)

### Market Data Collection

- WebSocket‑based data ingestion
- Binance Spot exchange
- Order book diff stream (`@depth@100ms`)
- Trade stream (`@trade`, upgradeable to `@aggTrade`)
- Local order book reconstruction using:
  - REST snapshot
  - WebSocket diffs
  - update‑ID sequencing
- Data persisted to CSV
- Berlin trading window: **08:00–22:00 Europe/Berlin**
- Dockerized & cron‑friendly

---

## Recorded Data Files

Per symbol, per day:

- `snapshots/snapshot_<event_id>_<tag>.csv`
- `orderbook_ws_depth_<SYMBOL>_<YYYYMMDD>.csv`
- `trades_ws_<SYMBOL>_<YYYYMMDD>.csv`
- `events_<SYMBOL>_<YYYYMMDD>.csv`
- `gaps_<SYMBOL>_<YYYYMMDD>.csv` (optional, for diagnostics)
- `diffs/depth_diffs_<SYMBOL>_<YYYYMMDD>.ndjson.gz` (optional raw WS diffs)

All numeric values are stored in **human‑readable fixed decimals**.

---

## Project Roadmap

### Phase 1 — Market Data
- [x] REST order book snapshot
- [x] WebSocket depth stream
- [x] Local order book reconstruction
- [x] Trade stream
- [ ] Switch to aggTrades
- [ ] Latency metrics
- [ ] Data validation

### Phase 2 — Backtesting Engine
- [x] Order book replay (snapshot + WS diffs)
- [x] Trade replay (trade stream)
- [x] Time‑aligned simulation (recv_ms merge)

### Phase 3 — Fill Models
- [x] Trade‑driven (trade_cross)
- [x] Poisson (Avellaneda–Stoikov)
- [x] Hybrid
- [ ] Price‑cross (L1/L2 cross-based)

### Phase 4 — Strategy Research
- [ ] Quoting logic
- [ ] Parameter calibration
- [ ] Inventory control
- [ ] PnL attribution

### Phase 3.5 — Calibration (Prerequisite)
- [x] Poisson fill calibration runners (ladder sweep and fixed-spread runs)
- [x] Backtest ingestion of calibration outputs (FILL_PARAMS_FILE)

### Phase 5 — Live / Testnet
- [ ] Testnet trading
- [ ] Shadow trading
- [ ] Monitoring
- [ ] Gradual production rollout

---

# Binance Market Data Recorder (Depth + Trades + Events Ledger)

One process per symbol. Produces the following per day:

- `orderbook_ws_depth_<SYMBOL>_<YYYYMMDD>.csv`: top-N bids/asks frames (only when book is synced)
- `trades_ws_<SYMBOL>_<YYYYMMDD>.csv`: trade prints with recv timestamps
- `events_<SYMBOL>_<YYYYMMDD>.csv`: authoritative ledger for run boundaries and sync/resync epochs
- `gaps_<SYMBOL>_<YYYYMMDD>.csv`: optional stream of gap/resync diagnostics
- `snapshots/snapshot_<event_id>_<tag>.csv`: REST snapshots referenced by `events.csv`
- `diffs/depth_diffs_<SYMBOL>_<YYYYMMDD>.ndjson.gz`: optional gzip’d raw WS diffs for exact replay

## Layout

```
data/<SYMBOL>/<YYYYMMDD>/
  orderbook_ws_depth_<SYMBOL>_<YYYYMMDD>.csv
  trades_ws_<SYMBOL>_<YYYYMMDD>.csv
  events_<SYMBOL>_<YYYYMMDD>.csv
  gaps_<SYMBOL>_<YYYYMMDD>.csv
  snapshots/
    snapshot_<event_id>_<tag>.csv
  diffs/
    depth_diffs_<SYMBOL>_<YYYYMMDD>.ndjson.gz

### File descriptions

- `orderbook_ws_depth_<SYMBOL>_<YYYYMMDD>.csv` — book frames (timestamps, run/epoch ids, top-N ladders).
- `trades_ws_<SYMBOL>_<YYYYMMDD>.csv` — WebSocket trades with event/recv timestamps, run id, price, quantity, aggressor flag.
- `events_<SYMBOL>_<YYYYMMDD>.csv` — lifecycle ledger documenting run start/stop, snapshot requests, loads, resyncs, and window boundaries.
- `gaps_<SYMBOL>_<YYYYMMDD>.csv` — optional diagnostics for gap detection, including timestamps and free-form details.
- `snapshots/` — tagged CSV snapshots keyed by run/event id, referenced by `events.csv`.
- `diffs/` — gzipped NDJSON stream of raw depth diffs for exact replay alignment.
```

## Run

```bash
pip install -r requirements.txt
SYMBOL=ETHUSDT python -m mm.market_data.recorder
```

Docker:

```bash
docker build -t mm-recorder:latest .
docker run --rm -e SYMBOL=ETHUSDT -v "$PWD/data":/app/data mm-recorder:latest
```

## Backtesting inputs

Load:
- `orderbook_ws_depth_<SYMBOL>_<YYYYMMDD>.csv` (filter `epoch_id >= 1`)
- `trades_ws_<SYMBOL>_<YYYYMMDD>.csv` (align by `event_time_ms`)
- `events_<SYMBOL>_<YYYYMMDD>.csv` (optional: segment by `epoch_id`, diagnose gaps, run boundaries)

---

## Folder READMEs

- `mm/market_data/README.md` — recorder configuration, environment variables, data layout, and how to orchestrate daily runs. Emphasizes the contract consumed by backtests.
- `mm/backtest/README.md` — describes the replay engines, how to point them at recorded datasets, and how to extend fill models or paper exchanges.

The recorder is the producer and the backtest is the consumer. Keep their shared `data/<SYMBOL>/<YYYYMMDD>/` structure intact so you can freely move sessions between live capture boxes and research machines.

---

## Running a backtest

```bash
pip install -r requirements.txt

# Example (spot): run a single day
DAY=20251216 SYMBOL=BTCUSDT python -m mm.runner_backtest
```

---

## Calibrating Poisson fill parameters (A,k)

Calibration is treated as a prerequisite step: it **produces** parameters which
later backtests **consume**.

Outputs are written under:

```
out/calibration/<method>/<SYMBOL>/<DAY>_<timestamp>/
  calibration_points.csv
  poisson_fit.json
  run_manifest.json
  (per-run folders with orders/fills/state)
```

### Design A: Ladder sweep (recommended for calibration)

Runs a single day replay while cycling through a list of deltas (`--deltas`) and
holding each delta for `--dwell-ms`.

```bash
pip install -r requirements.txt

python -m mm.calibration.runner_calibration \
  --method ladder \
  --symbol BTCUSDT \
  --day 20251216 \
  --deltas 1,2,3,5,8,13 \
  --dwell-ms 60000 \
  --fit-method poisson_mle
```

### Design B: Fixed-spread runs (easy operationally)

Runs multiple backtests (one per delta) and fits across runs.

```bash
python -m mm.calibration.runner_calibration \
  --method fixed \
  --symbol BTCUSDT \
  --day 20251216 \
  --deltas 1,2,3,5,8,13 \
  --fit-method poisson_mle
```

### Using calibration outputs in backtests

Point the backtest runner at the `poisson_fit.json` file:

```bash
export FILL_MODEL=poisson
export FILL_PARAMS_FILE=out/calibration/ladder/BTCUSDT/20251216_<timestamp>/poisson_fit.json

DAY=20251216 SYMBOL=BTCUSDT python -m mm.runner_backtest
```

Outputs are written to `out_backtest/` by default:

- `orders_<SYMBOL>.csv` — order lifecycle log (place/cancel/fill/close)
- `fills_<SYMBOL>.csv` — executed fills including fees
- `state_<SYMBOL>.csv` — inventory/cash/mark-to-market snapshots

See `mm/backtest/README.md` for environment variables controlling latency, TTL/GTC, tick size and initial balances.

## Running tests

```bash
pytest -q
```


## Walk-forward calibration (rolling window)

A rolling-window calibration + continuous backtest runner is available:

```bash
python -m mm.runner_walkforward --symbol BTCUSDT --day YYYYMMDD --data-root data --out-root out
```

See `mm/walkforward/README.md` for full details.
