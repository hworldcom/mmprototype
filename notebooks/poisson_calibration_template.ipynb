{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# --- Notebook bootstrap (works from repo root or notebooks/) ---\n",
    "REPO_ROOT = Path.cwd()\n",
    "if not (REPO_ROOT / 'mm').exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "DATA_ROOT = REPO_ROOT / 'data'\n",
    "OUT_ROOT = REPO_ROOT / 'out'\n",
    "\n",
    "print('REPO_ROOT:', REPO_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28194d",
   "metadata": {},
   "source": [
    "# Poisson Fill Model Calibration (Template)\n",
    "\n",
    "This notebook calibrates Poisson fill parameters for the model\n",
    "$$\\lambda(\\delta)=A e^{-k\\delta}$$\n",
    "from your backtest outputs:\n",
    "- `orders_<SYMBOL>.csv`\n",
    "- `fills_<SYMBOL>.csv`\n",
    "- `state_<SYMBOL>.csv`\n",
    "\n",
    "It estimates empirical fill intensities by quote distance (in **ticks**), fits `log(lambda)` vs `delta`, and outputs suggested `A` and `k`.\n",
    "\n",
    "> Assumption: you ran a baseline backtest that produces realistic fill events (typically `trade_driven`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025c9371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:\n",
      "  out_backtest/orders_BTCUSDT.csv\n",
      "  out_backtest/fills_BTCUSDT.csv\n",
      "  out_backtest/state_BTCUSDT.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(         recv_ms                              order_id      action  \\\n",
       " 0  1766127600134  8bf023bf-2502-4ae6-bb37-00544b009adc       PLACE   \n",
       " 1  1766127603417  8bf023bf-2502-4ae6-bb37-00544b009adc  CANCEL_REQ   \n",
       " 2  1766127603417  61b523bc-f9fe-4637-baa1-83b6152fe97f       PLACE   \n",
       " 3  1766127603517  8bf023bf-2502-4ae6-bb37-00544b009adc  CANCEL_ACK   \n",
       " 4  1766127604594  61b523bc-f9fe-4637-baa1-83b6152fe97f        FILL   \n",
       " \n",
       "            status side    price    qty  remaining_qty  active_recv_ms  \\\n",
       " 0            OPEN  BUY  87482.7  0.001        0.00100   1766127600184   \n",
       " 1  CANCEL_PENDING  BUY  87482.7  0.001        0.00100   1766127600184   \n",
       " 2            OPEN  BUY  87483.3  0.001        0.00100   1766127603467   \n",
       " 3       CANCELLED  BUY  87482.7  0.001        0.00100   1766127600184   \n",
       " 4            OPEN  BUY  87483.3  0.001        0.00094   1766127603467   \n",
       " \n",
       "    expire_recv_ms  cancel_req_ms  cancel_effective_ms  reject_reason  \\\n",
       " 0             NaN            NaN                  NaN            NaN   \n",
       " 1             NaN   1.766128e+12         1.766128e+12            NaN   \n",
       " 2             NaN            NaN                  NaN            NaN   \n",
       " 3             NaN   1.766128e+12         1.766128e+12            NaN   \n",
       " 4             NaN            NaN                  NaN            NaN   \n",
       " \n",
       "   cancel_reason  \n",
       " 0           NaN  \n",
       " 1  QUOTE_CHANGE  \n",
       " 2           NaN  \n",
       " 3  QUOTE_CHANGE  \n",
       " 4           NaN  ,\n",
       "          recv_ms                              order_id  side    price  \\\n",
       " 0  1766127604594  61b523bc-f9fe-4637-baa1-83b6152fe97f   BUY  87483.3   \n",
       " 1  1766127604594  61b523bc-f9fe-4637-baa1-83b6152fe97f   BUY  87483.3   \n",
       " 2  1766127604594  61b523bc-f9fe-4637-baa1-83b6152fe97f   BUY  87483.3   \n",
       " 3  1766127605820  f90b5878-eb21-4564-9c50-c2cbdb8d6cfc  SELL  87474.2   \n",
       " 4  1766127605820  f90b5878-eb21-4564-9c50-c2cbdb8d6cfc  SELL  87474.2   \n",
       " \n",
       "        qty       fee       reason  \n",
       " 0  0.00006  0.005249  trade_cross  \n",
       " 1  0.00012  0.010498  trade_cross  \n",
       " 2  0.00082  0.071736  trade_cross  \n",
       " 3  0.00006  0.005248  trade_cross  \n",
       " 4  0.00006  0.005248  trade_cross  ,\n",
       "          recv_ms  inventory    cash        mid  mtm_value  open_orders\n",
       " 0  1766127600134        0.0  1000.0  87483.405     1000.0            1\n",
       " 1  1766127600216        0.0  1000.0  87483.405     1000.0            1\n",
       " 2  1766127600316        0.0  1000.0  87483.405     1000.0            1\n",
       " 3  1766127600416        0.0  1000.0  87483.405     1000.0            1\n",
       " 4  1766127600516        0.0  1000.0  87483.405     1000.0            1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- User inputs ---\n",
    "OUT_DIR = Path(os.environ.get('OUT_DIR', 'out_backtest'))\n",
    "SYMBOL = os.environ.get('SYMBOL', 'BTCUSDT')\n",
    "\n",
    "# Tick size must match what you used in the backtest\n",
    "TICK_SIZE = float(os.environ.get('TICK_SIZE', '0.01'))\n",
    "\n",
    "# Optional: exclude orders too close to mid where queue effects dominate\n",
    "MIN_DELTA_TICKS = int(os.environ.get('MIN_DELTA_TICKS', '0'))\n",
    "\n",
    "# Optional: bucket cap (avoid long tails)\n",
    "MAX_DELTA_TICKS = int(os.environ.get('MAX_DELTA_TICKS', '50'))\n",
    "\n",
    "# Minimum exposure (seconds) per bucket required to include it in the fit\n",
    "MIN_EXPOSURE_S = float(os.environ.get('MIN_EXPOSURE_S', '5.0'))\n",
    "\n",
    "orders_path = resolve_csv_path(OUT_DIR / f'orders_{SYMBOL}.csv')\n",
    "fills_path  = resolve_csv_path(OUT_DIR / f'fills_{SYMBOL}.csv')\n",
    "state_path  = resolve_csv_path(OUT_DIR / f'state_{SYMBOL}.csv')\n",
    "\n",
    "print('Loading:')\n",
    "print(' ', orders_path)\n",
    "print(' ', fills_path)\n",
    "print(' ', state_path)\n",
    "\n",
    "orders = pd.read_csv(orders_path)\n",
    "fills  = pd.read_csv(fills_path)\n",
    "state  = pd.read_csv(state_path)\n",
    "\n",
    "orders.head(), fills.head(), state.head()\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def resolve_csv_path(p: Path) -> Path:\n",
    "    \"\"\"Return p if exists; otherwise try p+'.gz'. Accepts either .csv or .csv.gz.\"\"\"\n",
    "    if p.exists():\n",
    "        return p\n",
    "    gz = Path(str(p) + '.gz')\n",
    "    if gz.exists():\n",
    "        return gz\n",
    "    # also allow replacing suffix if user provided .csv.gz already\n",
    "    if p.suffix == '.gz' and p.exists():\n",
    "        return p\n",
    "    raise FileNotFoundError(f'File not found: {p} (or {gz})')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30fe6f1",
   "metadata": {},
   "source": [
    "## Build per-order exposure records\n",
    "\n",
    "For each placed order we compute:\n",
    "- `active_ms` (when it becomes fill-eligible)\n",
    "- `end_ms` = min(first fill time, cancel effective time, end of run)\n",
    "- `exposure_s` = (end_ms - active_ms)/1000\n",
    "- `delta_ticks` = distance from mid at activation time\n",
    "\n",
    "Then we aggregate by `delta_ticks` bucket:\n",
    "- total exposure time\n",
    "- number of fill events (or filled qty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure correct dtypes\n",
    "for col in ['recv_ms','active_recv_ms','expire_recv_ms','cancel_effective_ms']:\n",
    "    if col in orders.columns:\n",
    "        orders[col] = pd.to_numeric(orders[col], errors='coerce')\n",
    "\n",
    "fills['recv_ms'] = pd.to_numeric(fills['recv_ms'], errors='coerce')\n",
    "state['recv_ms'] = pd.to_numeric(state['recv_ms'], errors='coerce')\n",
    "\n",
    "# Consider only PLACE events as order creations\n",
    "placed = orders[orders['action'] == 'PLACE'].copy()\n",
    "\n",
    "# Drop rejected placements if present\n",
    "if 'status' in placed.columns:\n",
    "    placed = placed[placed['status'] != 'REJECTED'].copy()\n",
    "\n",
    "# Active time: if missing, fall back to recv_ms\n",
    "placed['active_ms'] = placed['active_recv_ms'].fillna(placed['recv_ms']).astype('int64')\n",
    "\n",
    "# Determine an end-of-run timestamp\n",
    "end_run_ms = int(max(state['recv_ms'].max(), orders['recv_ms'].max(), fills['recv_ms'].max()))\n",
    "print('End of run ms:', end_run_ms)\n",
    "\n",
    "# First fill time per order\n",
    "first_fill_ms = fills.groupby('order_id', as_index=False)['recv_ms'].min().rename(columns={'recv_ms':'first_fill_ms'})\n",
    "\n",
    "# Cancel effective time per order (from CANCEL_REQ rows)\n",
    "cancel_req = orders[orders['action'] == 'CANCEL_REQ'].copy()\n",
    "if 'cancel_effective_ms' in cancel_req.columns:\n",
    "    cancel_eff = cancel_req.groupby('order_id', as_index=False)['cancel_effective_ms'].min().rename(columns={'cancel_effective_ms':'cancel_eff_ms'})\n",
    "else:\n",
    "    cancel_eff = pd.DataFrame(columns=['order_id','cancel_eff_ms'])\n",
    "\n",
    "# Merge into placed\n",
    "placed = placed.merge(first_fill_ms, on='order_id', how='left')\n",
    "placed = placed.merge(cancel_eff, on='order_id', how='left')\n",
    "\n",
    "# Compute end time = min(first fill, cancel effective, end)\n",
    "placed['end_ms'] = end_run_ms\n",
    "mask = placed['first_fill_ms'].notna()\n",
    "placed.loc[mask, 'end_ms'] = np.minimum(placed.loc[mask, 'end_ms'], placed.loc[mask, 'first_fill_ms'])\n",
    "mask = placed['cancel_eff_ms'].notna()\n",
    "placed.loc[mask, 'end_ms'] = np.minimum(placed.loc[mask, 'end_ms'], placed.loc[mask, 'cancel_eff_ms'])\n",
    "\n",
    "# Exposure time in seconds (clip at >=0)\n",
    "placed['exposure_s'] = (placed['end_ms'] - placed['active_ms']).clip(lower=0) / 1000.0\n",
    "\n",
    "# Mid at activation time: merge_asof\n",
    "state_sorted = state[['recv_ms','mid']].sort_values('recv_ms')\n",
    "placed_sorted = placed.sort_values('active_ms')\n",
    "mid_at_active = pd.merge_asof(\n",
    "    placed_sorted,\n",
    "    state_sorted,\n",
    "    left_on='active_ms',\n",
    "    right_on='recv_ms',\n",
    "    direction='backward'\n",
    ").rename(columns={'mid':'mid_at_active'})\n",
    "\n",
    "# Distance in ticks\n",
    "mid_at_active['delta_ticks'] = (np.abs(mid_at_active['price'] - mid_at_active['mid_at_active']) / TICK_SIZE)\n",
    "mid_at_active['delta_bucket'] = np.floor(mid_at_active['delta_ticks']).astype(int)\n",
    "\n",
    "# Filter buckets\n",
    "mid_at_active = mid_at_active[(mid_at_active['delta_bucket'] >= MIN_DELTA_TICKS) & (mid_at_active['delta_bucket'] <= MAX_DELTA_TICKS)].copy()\n",
    "\n",
    "mid_at_active[['order_id','side','price','mid_at_active','delta_ticks','delta_bucket','exposure_s']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412372cd",
   "metadata": {},
   "source": [
    "## Empirical intensity by distance bucket\n",
    "\n",
    "Two common choices:\n",
    "- **Event intensity**: number of fill events per second (closer to the AS arrival-rate assumption)\n",
    "- **Size intensity**: filled quantity per second\n",
    "\n",
    "This template computes **event intensity** by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9732c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill event counts and filled qty per order\n",
    "fill_event_counts = fills.groupby('order_id').size().rename('fill_events').reset_index()\n",
    "filled_qty = fills.groupby('order_id', as_index=False)['qty'].sum().rename(columns={'qty':'filled_qty'})\n",
    "\n",
    "df = mid_at_active.merge(fill_event_counts, on='order_id', how='left').merge(filled_qty, on='order_id', how='left')\n",
    "df['fill_events'] = df['fill_events'].fillna(0).astype(int)\n",
    "df['filled_qty'] = df['filled_qty'].fillna(0.0)\n",
    "\n",
    "# Aggregate by delta bucket\n",
    "agg = df.groupby('delta_bucket', as_index=False).agg(\n",
    "    exposure_s=('exposure_s','sum'),\n",
    "    fill_events=('fill_events','sum'),\n",
    "    filled_qty=('filled_qty','sum'),\n",
    "    n_orders=('order_id','count')\n",
    ")\n",
    "\n",
    "agg['lambda_events_per_s'] = agg['fill_events'] / agg['exposure_s'].replace(0, np.nan)\n",
    "agg['lambda_qty_per_s'] = agg['filled_qty'] / agg['exposure_s'].replace(0, np.nan)\n",
    "\n",
    "fit_df = agg[(agg['exposure_s'] >= MIN_EXPOSURE_S) & (agg['lambda_events_per_s'] > 0)].copy()\n",
    "\n",
    "agg.head(15), fit_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(agg['delta_bucket'], agg['lambda_events_per_s'], marker='o')\n",
    "plt.xlabel('delta (ticks)')\n",
    "plt.ylabel('empirical lambda (fill events / second)')\n",
    "plt.title('Empirical Fill Intensity by Quote Distance')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf4c88",
   "metadata": {},
   "source": [
    "## Fit `log(lambda)` vs `delta`\n",
    "\n",
    "We fit:\n",
    "$$\\log \\lambda(\\delta) = \\log A - k\\delta$$\n",
    "\n",
    "Weighted by exposure time (more exposure \u21d2 more reliable estimate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(fit_df) < 2:\n",
    "    raise RuntimeError('Not enough buckets with positive intensity and sufficient exposure to fit. Try lowering MIN_EXPOSURE_S or increasing MAX_DELTA_TICKS.')\n",
    "\n",
    "x = fit_df['delta_bucket'].to_numpy(dtype=float)\n",
    "y = np.log(fit_df['lambda_events_per_s'].to_numpy(dtype=float))\n",
    "w = fit_df['exposure_s'].to_numpy(dtype=float)\n",
    "\n",
    "# Weighted linear fit: y = b0 + b1*x, where b1 = -k and b0 = log(A)\n",
    "b1, b0 = np.polyfit(x, y, deg=1, w=w)\n",
    "k_hat = -b1\n",
    "A_hat = float(np.exp(b0))\n",
    "\n",
    "print('Fitted parameters (event intensity):')\n",
    "print('  A =', A_hat)\n",
    "print('  k =', k_hat)\n",
    "\n",
    "x_line = np.linspace(x.min(), x.max(), 100)\n",
    "y_line = b0 + b1 * x_line\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x, y, s=40)\n",
    "plt.plot(x_line, y_line)\n",
    "plt.xlabel('delta (ticks)')\n",
    "plt.ylabel('log(lambda)')\n",
    "plt.title('Log-Intensity Fit: log(lambda)=log(A)-k*delta')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46d599",
   "metadata": {},
   "source": [
    "## Suggested `FILL_PARAMS_JSON`\n",
    "\n",
    "Set `dt_ms` to a reasonably small step (commonly 50\u2013200 ms). If unsure, start with 100 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b1a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_ms = int(os.environ.get('POISSON_DT_MS', '100'))\n",
    "params = {'A': float(A_hat), 'k': float(k_hat), 'dt_ms': dt_ms}\n",
    "print('FILL_MODEL=poisson')\n",
    "print('FILL_PARAMS_JSON=' + json.dumps(params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75caf56",
   "metadata": {},
   "source": [
    "## Notes / interpretation\n",
    "\n",
    "- `A` is the baseline fill-event rate near the mid (units: events/sec).\n",
    "- `k` controls how quickly the fill rate decays as you quote wider.\n",
    "- If you see implausibly large `A`, consider excluding very small deltas (`MIN_DELTA_TICKS=1` or `2`) and re-fitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}